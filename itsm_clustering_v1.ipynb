{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0936b4-2bb4-4e88-994a-36d1ba38ae90",
   "metadata": {},
   "source": [
    "## **Incident Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67da0739-df90-41ae-b918-14d2f2837d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing to: /home/jovyan/.conda/envs/default/bin/python\n",
      "Packages will go to: /home/jovyan/.conda/envs/default/lib/python3.12/site-packages\n",
      "Looking in indexes: https://pypi.celonis.cloud, https://pypi.org/simple\n",
      "Collecting numpy==2.2.4\n",
      "  Using cached numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas==2.2.3\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting scikit-learn==1.6.1\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting sentence-transformers==4.1.0\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch==2.6.0\n",
      "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting transformers==4.51.3\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pycelonis==2.13.0\n",
      "  Using cached https://pypi.celonis.cloud/pycelonis/pycelonis-2.13.0-py3-none-any.whl (255 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.3)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.3)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.3)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.6.1)\n",
      "  Using cached scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.6.1)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.6.1)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tqdm (from sentence-transformers==4.1.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers==4.1.0)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence-transformers==4.1.0)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers==4.1.0)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from torch==2.6.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting networkx (from torch==2.6.0)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.6.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.6.0)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0)\n",
      "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting setuptools (from torch==2.6.0)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting packaging>=20.0 (from transformers==4.51.3)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.51.3)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.51.3)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers==4.51.3)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.51.3)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pycelonis-core>=2.10.3 (from pycelonis==2.13.0)\n",
      "  Using cached https://pypi.celonis.cloud/pycelonis-core/pycelonis_core-2.10.3-py3-none-any.whl (21 kB)\n",
      "Collecting pyarrow>=8.0.0 (from pycelonis==2.13.0)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting saolapy>=0.3.3.dev0 (from pycelonis==2.13.0)\n",
      "  Using cached https://pypi.celonis.cloud/saolapy/saolapy-0.3.3-py3-none-any.whl (41 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers==4.1.0)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting httpx>=0.28.0 (from pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic (from pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.3)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.51.3)\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.51.3)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.51.3)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.51.3)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting anyio (from httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: triton, pytz, nvidia-cusparselt-cu12, mpmath, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, sympy, sniffio, six, setuptools, safetensors, regex, pyyaml, pyarrow, Pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, h11, fsspec, filelock, charset_normalizer, certifi, annotated-types, typing-inspection, scipy, requests, python-dateutil, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, httpcore, anyio, scikit-learn, pydantic, pandas, nvidia-cusolver-cu12, huggingface-hub, httpx, torch, tokenizers, saolapy, pycelonis-core, transformers, pycelonis, sentence-transformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.6.0\n",
      "    Uninstalling threadpoolctl-3.6.0:\n",
      "      Successfully uninstalled threadpoolctl-3.6.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 80.9.0\n",
      "    Uninstalling setuptools-80.9.0:\n",
      "      Successfully uninstalled setuptools-80.9.0\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 20.0.0\n",
      "    Uninstalling pyarrow-20.0.0:\n",
      "      Successfully uninstalled pyarrow-20.0.0\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 11.2.1\n",
      "    Uninstalling pillow-11.2.1:\n",
      "      Successfully uninstalled pillow-11.2.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.5\n",
      "    Uninstalling networkx-3.5:\n",
      "      Successfully uninstalled networkx-3.5\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.5.1\n",
      "    Uninstalling joblib-1.5.1:\n",
      "      Successfully uninstalled joblib-1.5.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.1.5\n",
      "    Uninstalling hf-xet-1.1.5:\n",
      "      Successfully uninstalled hf-xet-1.1.5\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.6.15\n",
      "    Uninstalling certifi-2025.6.15:\n",
      "      Successfully uninstalled certifi-2025.6.15\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: typing-inspection\n",
      "    Found existing installation: typing-inspection 0.4.1\n",
      "    Uninstalling typing-inspection-0.4.1:\n",
      "      Successfully uninstalled typing-inspection-0.4.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.0\n",
      "    Uninstalling scipy-1.16.0:\n",
      "      Successfully uninstalled scipy-1.16.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.2\n",
      "    Uninstalling pydantic_core-2.33.2:\n",
      "      Successfully uninstalled pydantic_core-2.33.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.9.0\n",
      "    Uninstalling anyio-4.9.0:\n",
      "      Successfully uninstalled anyio-4.9.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.7\n",
      "    Uninstalling pydantic-2.11.7:\n",
      "      Successfully uninstalled pydantic-2.11.7\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: saolapy\n",
      "    Found existing installation: saolapy 0.3.3\n",
      "    Uninstalling saolapy-0.3.3:\n",
      "      Successfully uninstalled saolapy-0.3.3\n",
      "  Attempting uninstall: pycelonis-core\n",
      "    Found existing installation: pycelonis_core 2.10.3\n",
      "    Uninstalling pycelonis_core-2.10.3:\n",
      "      Successfully uninstalled pycelonis_core-2.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: pycelonis\n",
      "    Found existing installation: pycelonis 2.13.0\n",
      "    Uninstalling pycelonis-2.13.0:\n",
      "      Successfully uninstalled pycelonis-2.13.0\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 4.1.0\n",
      "    Uninstalling sentence-transformers-4.1.0:\n",
      "      Successfully uninstalled sentence-transformers-4.1.0\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.7.14 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.7.0 h11-0.16.0 hf-xet-1.1.5 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.33.4 idna-3.10 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 numpy-2.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 packaging-25.0 pandas-2.2.3 pyarrow-21.0.0 pycelonis-2.13.0 pycelonis-core-2.10.3 pydantic-2.11.7 pydantic-core-2.33.2 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 saolapy-0.3.3 scikit-learn-1.6.1 scipy-1.16.0 sentence-transformers-4.1.0 setuptools-80.9.0 six-1.17.0 sniffio-1.3.1 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3 triton-3.2.0 typing-inspection-0.4.1 typing_extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNCOMMENT AND RUN THIS THE FIRST TIME YOUR RUN THE SCRIPT, THEN COMMENT IT BACK\n",
    "# Install packages to the correct location (3.12 site-packages)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Installing to: {sys.executable}\")\n",
    "print(\"Packages will go to: /home/jovyan/.conda/envs/default/lib/python3.12/site-packages\")\n",
    "\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'pip', 'install', '--force-reinstall',\n",
    "    'numpy==2.2.4',\n",
    "    'pandas==2.2.3', \n",
    "    'scikit-learn==1.6.1',\n",
    "    'sentence-transformers==4.1.0',\n",
    "    'torch==2.6.0',\n",
    "    'transformers==4.51.3',\n",
    "    'pycelonis==2.13.0'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a5939a-2607-4629-879c-a4253d05999c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from pycelonis import get_celonis\n",
    "import pycelonis.pql as pql\n",
    "from pycelonis.config import Config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b2036e-55b2-4f80-b992-b4636e24db43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_common_embedding(group):\n",
    "    if group.shape[0] < 2: \n",
    "        return group\n",
    "    common_embedding = group.iloc[0, df_number_of_cols:].values  # Use the first embedding in the group (all columns except 'ManufacturerPartNumber')\n",
    "    for i in range(embeddings_reduced.shape[1]):\n",
    "        group.iloc[:, df_number_of_cols+i] = common_embedding[i-1]\n",
    "    return group\n",
    "\n",
    "def hack_embeddings(column_name):\n",
    "    df_withcol = df[df[column_name].notna()]\n",
    "    df_withoutcol = df[df[column_name].isna()]\n",
    "    df_withcol = df_withcol.groupby(column_name).apply(assign_common_embedding).reset_index(drop=True)\n",
    "    return pd.concat((df_withcol, df_withoutcol), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62c3d8-aa8e-449f-b1a6-1d625d75ad54",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **1.Get Data from Celonis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb012b31-0bbd-44d0-ae3b-5b5153ab4c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 22:18:42,125] INFO: No `base_url` given. Using environment variable 'CELONIS_URL'\n",
      "[2025-07-21 22:18:42,126] INFO: No `api_token` given. Using environment variable 'CELONIS_API_TOKEN'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 22:18:42,455] WARNING: Your PyCelonis Version 2.13.0 is outdated (Newest Version: 2.14.0). Please upgrade the package via: pip install --extra-index-url=https://pypi.celonis.cloud/ pycelonis pycelonis_core --upgrade\n",
      "[2025-07-21 22:18:43,176] WARNING: KeyType is not set. Defaulted to 'APP_KEY'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 22:18:43,227] INFO: Initial connect successful! PyCelonis Version: 2.13.0\n",
      "[2025-07-21 22:18:43,557] INFO: `ml-workbench` permissions: ['DELETE_SCHEDULERS', 'EDIT_SCHEDULERS', 'USE_ALL_SCHEDULERS', 'CREATE_APPS', 'USE_ALL_APPS', 'CREATE_SCHEDULERS', 'CREATE_WORKSPACES', 'MANAGE_ALL_APPS', 'MANAGE_ALL_WORKSPACES', 'MANAGE_SCHEDULERS_PERMISSIONS', 'VIEW_CONFIGURATION']\n",
      "[2025-07-21 22:18:43,558] INFO: `team` permissions: []\n",
      "[2025-07-21 22:18:43,558] INFO: `process-repository` permissions: []\n",
      "[2025-07-21 22:18:43,558] INFO: `storage-manager` permissions: []\n",
      "[2025-07-21 22:18:43,559] INFO: `package-manager` permissions: []\n",
      "[2025-07-21 22:18:43,559] INFO: `transformation-center` permissions: []\n",
      "[2025-07-21 22:18:43,560] INFO: `compute-live` permissions: []\n",
      "[2025-07-21 22:18:43,560] INFO: `task-mining` permissions: []\n",
      "[2025-07-21 22:18:43,560] INFO: `event-collection` permissions: ['EDIT_ALL_DATA_POOLS_RESTRICTED', 'USE_ALL_DATA_MODELS', 'VIEW_ALL_DATA_POOLS', 'CREATE_DATA_POOL', 'EDIT_ALL_DATA_POOLS']\n",
      "[2025-07-21 22:18:43,560] INFO: `transformation-hub` permissions: []\n",
      "[2025-07-21 22:18:43,561] INFO: `user-provisioning` permissions: []\n",
      "[2025-07-21 22:18:43,561] INFO: `workflows` permissions: []\n"
     ]
    }
   ],
   "source": [
    "# Get Data from Celonis\n",
    "\n",
    "data_pool = 'OCPM Data Pool [ARM Multi-Object DEV]' #data pool where the data resides\n",
    "data_model = 'perspective_custom_ITSM'\n",
    "\n",
    "celonis = get_celonis()\n",
    "pool = celonis.data_integration.get_data_pools().find(data_pool)\n",
    "dm = pool.get_data_models().find(data_model)\n",
    "Config.DEFAULT_DATA_MODEL = dm\n",
    "\n",
    "# Choose if Data Model should be reloaded (True) or not (False)\n",
    "UPDATE_DM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21eb83d1-6556-49f1-a7d7-bcecf8f56939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Analysis:\n",
      "o_custom_RequestItem\n",
      "o_custom_Sla\n",
      "o_custom_AssignmentGroup\n",
      "o_custom_Incident\n",
      "o_custom_CatalogTask\n",
      "o_custom_Problem\n",
      "o_custom_ProblemTask\n",
      "o_custom_Problem__AssignmentGroup\n",
      "o_custom_CatalogTask__Sla\n",
      "o_custom_Cluster\n",
      "\n",
      "\n",
      "Column Analysis:\n",
      "o_custom_Incident columns (30 total):\n",
      "\n",
      "Number\n",
      "ProblemNumber\n",
      "ChangeRequestNumber\n",
      "AssignmentGroupName\n",
      "AssignedTo\n",
      "ReassignmentCount\n",
      "OpenedBy\n",
      "ResolvedBy\n",
      "ClosedBy\n",
      "ContactType\n",
      "Category\n",
      "SubCategory\n",
      "ConfigurationItem\n",
      "BusinessService\n",
      "Location\n",
      "ShortDescription\n",
      "Description\n",
      "Priority\n",
      "Impact\n",
      "Urgency\n",
      "CloseCode\n",
      "CloseNotes\n",
      "ReopenCount\n",
      "State\n",
      "OpenedAt\n",
      "ClosedAt\n",
      "ID\n",
      "AssignmentGroup_ID\n",
      "Problem_ID\n",
      "Cluster_ID\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows in the DM except event tables, relationship tables and tables starting with _\n",
    "print(f\"Table Analysis:\")\n",
    "for tab in dm.get_tables():\n",
    "    if not (tab.name.startswith('e') or tab.name.startswith('r') or tab.name.startswith('_')):\n",
    "        print(tab.alias)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Input the tables aliases as they are in the DM\n",
    "table_aliases = ['o_custom_Incident']\n",
    "tables = {table_alias:[x for x in dm.get_tables() if x.alias == table_alias][0] for table_alias in table_aliases} \n",
    "tables\n",
    "\n",
    "# Inspect columns\n",
    "columns = {table_name:table.get_columns() for table_name,table in tables.items()}\n",
    "\n",
    "print(f\"Column Analysis:\")\n",
    "for table_name, cols in columns.items():\n",
    "    print(table_name + f\" columns ({len(cols)} total):\\n\")\n",
    "    for col in cols:\n",
    "        print(col.name)\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a652fee0-ff9b-4649-a0c9-2487ff942c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 22:18:45,567] INFO: Successfully created data export using api v1 with id 'e6842172-11ea-449f-9019-daaa4e3706a9'\n",
      "[2025-07-21 22:18:45,568] INFO: Wait for execution of data export with id 'e6842172-11ea-449f-9019-daaa4e3706a9'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567d20f6498349889e2e9e4ebfd89c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 22:18:45,673] INFO: Export result chunks for data export with id 'e6842172-11ea-449f-9019-daaa4e3706a9'\n",
      "\n",
      "\n",
      "Memory usage:  0.70 MB\n",
      "Nr. of records: 43542\n",
      "Nr. of unique short descriptions: 43542\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>freetext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000725eac3d0aed098983b3c0501312f</td>\n",
       "      <td>[PRTG Network Monitor (GB-REH-PRTG-E1)] 4 Summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009caba1b431e5093a2ea88b04bcb25</td>\n",
       "      <td>Envoy printer (brother QL-820NWB) keep power o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0017563a1bb85e5068398557d34bcb4e</td>\n",
       "      <td>CRITICAL: Worker unresponsive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0027f0021b33d21c2380404cd34bcb0f</td>\n",
       "      <td>LSF - LSF job existing while checking out hspi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004243c41b0d5e902380404cd34bcb3e</td>\n",
       "      <td>CMBC-1-82 - Scheduling panel offline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00677475c3842650661799fb050131c4</td>\n",
       "      <td>Aircon issue reported in Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>007803a11b9d921068398557d34bcbe5</td>\n",
       "      <td>Wiki.arm.com is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0086ea9a1b711e1868398557d34bcb43</td>\n",
       "      <td>Meeting Room – Issue - MUC-4-Starnberger See</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00d6257895f9e61007a15b5ad9eabf7f</td>\n",
       "      <td>I would like to raise a ticket for laptop rais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0120987f1baa5ad02380404cd34bcb3f</td>\n",
       "      <td>Queue events-data-product-entitlement-updated-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0  000725eac3d0aed098983b3c0501312f   \n",
       "1  0009caba1b431e5093a2ea88b04bcb25   \n",
       "2  0017563a1bb85e5068398557d34bcb4e   \n",
       "3  0027f0021b33d21c2380404cd34bcb0f   \n",
       "4  004243c41b0d5e902380404cd34bcb3e   \n",
       "5  00677475c3842650661799fb050131c4   \n",
       "6  007803a11b9d921068398557d34bcbe5   \n",
       "7  0086ea9a1b711e1868398557d34bcb43   \n",
       "8  00d6257895f9e61007a15b5ad9eabf7f   \n",
       "9  0120987f1baa5ad02380404cd34bcb3f   \n",
       "\n",
       "                                            freetext  \n",
       "0  [PRTG Network Monitor (GB-REH-PRTG-E1)] 4 Summ...  \n",
       "1  Envoy printer (brother QL-820NWB) keep power o...  \n",
       "2                      CRITICAL: Worker unresponsive  \n",
       "3  LSF - LSF job existing while checking out hspi...  \n",
       "4               CMBC-1-82 - Scheduling panel offline  \n",
       "5                 Aircon issue reported in Cambridge  \n",
       "6                               Wiki.arm.com is down  \n",
       "7       Meeting Room – Issue - MUC-4-Starnberger See  \n",
       "8  I would like to raise a ticket for laptop rais...  \n",
       "9  Queue events-data-product-entitlement-updated-...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pql_filter = '''FILTER DAYS_BETWEEN(\"o_custom_Incident\".\"OpenedAt\", MINUTE_NOW()) <= 365'''    # change this if you want to include a filter\n",
    "\n",
    "# there are two ways to get data: either via simple columns/attributes or via custom PQL\n",
    "# simple column example: columns['MARA'].find(\"MANDT\") would give you MARA.MANDT\n",
    "df_meta = pql.DataFrame(\n",
    "    data=\n",
    "    {\n",
    "        \"ID\": columns['o_custom_Incident'].find(\"ID\")\n",
    "        ,\"freetext\": columns['o_custom_Incident'].find(\"ShortDescription\")\n",
    "    },\n",
    "    filters=\n",
    "    {pql_filter}\n",
    ")\n",
    "\n",
    "try:\n",
    "    df = df_meta.to_pandas().reset_index(drop=True)\n",
    "except:\n",
    "    df = pd.read_csv('materials-dataset.csv')\n",
    "    df.columns = ['ID','freetext']\n",
    "    \n",
    "df = df.drop_duplicates().dropna(subset=['ID'])\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1e6: .2f} MB\")\n",
    "print(\"Nr. of records:\", len(df))\n",
    "print(\"Nr. of unique short descriptions:\", len(df[\"ID\"].unique()))\n",
    "print(f\"\\n\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde7c5f-71f7-4295-af93-e0029cb47835",
   "metadata": {},
   "source": [
    "#### **2. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d973bad3-4f71-4c8a-b7ae-e7acd6450ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare freetext column to be fed to the LLM (i.e. set the freetext column as a concatentation of columns from the data above, use the column names as in row[column_name])\n",
    "df[\"freetext\"] = df.apply(lambda row: f\"{row['freetext']}\", axis=1)\n",
    "df['freetext'] = df['freetext'].fillna('')\n",
    "#df\n",
    "\n",
    "# Needed for correct concatenation of columns later\n",
    "df_number_of_cols = df.shape[1]\n",
    "#df_number_of_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c9e74-2b28-404d-af94-6e9a8d8174b1",
   "metadata": {},
   "source": [
    "#### **3. Clustering using Embeddings + Additional Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e94b48-d934-414f-a1b0-0b5f2aba3836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8068879f5375434ea1982f1c6a151512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape:  torch.Size([43542, 768])\n"
     ]
    }
   ],
   "source": [
    "# Run transformer\n",
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\")  # previously: all-MiniLM-L6-v2\n",
    "embeddings = model.encode(df.freetext, batch_size=256, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(\"Embeddings shape: \", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92eb5cb4-bd3b-446f-af00-81325f0197a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters...\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING RESULTS:\n",
      "================================================================================\n",
      " min_samples  pca_components  eps  noise_pct  n_clusters  avg_cluster_size  tiny_cluster_pct  median_cluster_size  largest_cluster  cluster_size_std  variance_explained\n",
      "           2              75 0.20       44.8        1505              16.0              73.6                  3.0             1916              81.8               0.821\n",
      "           2              75 0.25       43.5        1452              16.9              74.7                  3.0             1916              91.3               0.821\n",
      "           2              75 0.30       41.8        1425              17.8              74.9                  2.0             2190             104.6               0.821\n",
      "           2              75 0.35       39.8        1446              18.1              77.5                  2.0             2391             114.0               0.821\n",
      "           2              75 0.40       37.6        1461              18.6              78.9                  2.0             2395             120.5               0.821\n",
      "           2              75 0.45       34.5        1449              19.7              79.8                  2.0             3160             144.6               0.821\n",
      "           2              75 0.50       30.2        1419              21.4              81.8                  2.0             3328             174.8               0.821\n",
      "           2              75 0.60       17.7         950              37.7              84.0                  2.0            11171             414.0               0.821\n",
      "           2              75 0.65       10.2         571              68.5              87.7                  2.0            17981             794.8               0.821\n",
      "           2              75 0.70        3.9         165             253.7              81.8                  2.0            29229            2299.1               0.821\n",
      "           2             100 0.20       45.5        1616              14.7              72.1                  3.0             1916              77.7               0.884\n",
      "           2             100 0.25       44.4        1462              16.6              74.4                  3.0             1916              88.7               0.884\n",
      "           2             100 0.30       43.1        1420              17.5              74.9                  3.0             2033              96.3               0.884\n",
      "           2             100 0.35       41.4        1411              18.1              75.3                  2.0             2190             108.3               0.884\n",
      "           2             100 0.40       39.5        1432              18.4              77.8                  2.0             2395             115.4               0.884\n",
      "           2             100 0.45       37.4        1439              18.9              78.8                  2.0             2395             126.0               0.884\n",
      "           2             100 0.50       34.6        1419              20.1              80.3                  2.0             3160             152.2               0.884\n",
      "           2             100 0.60       26.0        1256              25.7              82.3                  2.0             5499             226.1               0.884\n",
      "           2             100 0.65       20.2        1026              33.9              84.1                  2.0             9728             356.7               0.884\n",
      "           2             100 0.70       13.9         681              55.1              86.0                  2.0            18448             737.2               0.884\n",
      "           2             125 0.20       45.9        1651              14.3              71.8                  3.0             1916              76.8               0.928\n",
      "           2             125 0.25       44.8        1478              16.2              73.6                  3.0             1916              85.3               0.928\n",
      "           2             125 0.30       43.7        1435              17.1              75.0                  3.0             2033              94.9               0.928\n",
      "           2             125 0.35       42.3        1384              18.1              75.2                  3.0             2190             108.0               0.928\n",
      "           2             125 0.40       40.7        1386              18.6              76.8                  2.0             2391             116.2               0.928\n",
      "           2             125 0.45       38.8        1425              18.7              78.6                  2.0             2395             119.5               0.928\n",
      "           2             125 0.50       36.6        1424              19.4              80.1                  2.0             3160             140.6               0.928\n",
      "           2             125 0.60       29.9        1315              23.2              80.9                  2.0             3888             190.2               0.928\n",
      "           2             125 0.65       25.3        1173              27.7              83.4                  2.0             6168             250.9               0.928\n",
      "           2             125 0.70       19.9         955              36.5              83.7                  2.0            10009             381.1               0.928\n",
      "           3              75 0.20       48.0         807              28.0              50.7                  5.0             1916             110.3               0.821\n",
      "           3              75 0.25       46.7         755              30.7              51.3                  5.0             1916             125.0               0.821\n",
      "           3              75 0.30       45.0         712              33.6              49.9                  6.0             2190             146.3               0.821\n",
      "           3              75 0.35       43.3         686              36.0              52.5                  5.0             2391             163.7               0.821\n",
      "           3              75 0.40       41.1         690              37.1              55.2                  5.0             2395             173.6               0.821\n",
      "           3              75 0.45       38.1         660              40.8              55.6                  5.0             3160             212.4               0.821\n",
      "           3              75 0.50       33.8         638              45.2              59.6                  4.0             3328             258.8               0.821\n",
      "           3              75 0.60       20.2         402              86.4              62.2                  4.0            11171             633.6               0.821\n",
      "           3              75 0.65       11.8         204             188.2              65.7                  4.0            17981            1323.5               0.821\n",
      "           3              75 0.70        4.4          59             705.8              49.2                  6.0            29229            3824.0               0.821\n",
      "           3             100 0.20       48.9         883              25.2              48.9                  6.0             1916             103.9               0.884\n",
      "           3             100 0.25       47.6         763              29.9              50.9                  5.0             1916             121.3               0.884\n",
      "           3             100 0.30       46.2         732              32.0              51.2                  5.0             2033             132.5               0.884\n",
      "           3             100 0.35       44.7         697              34.6              50.1                  5.0             2190             152.4               0.884\n",
      "           3             100 0.40       43.0         681              36.4              53.3                  5.0             2395             165.6               0.884\n",
      "           3             100 0.45       40.9         682              37.7              55.3                  5.0             2395             181.3               0.884\n",
      "           3             100 0.50       38.1         654              41.2              57.2                  5.0             3160             222.5               0.884\n",
      "           3             100 0.60       29.2         549              56.2              59.6                  4.0             5499             339.8               0.884\n",
      "           3             100 0.65       22.8         449              74.8              63.7                  4.0             9728             536.8               0.884\n",
      "           3             100 0.70       15.9         244             150.1              61.1                  4.0            18448            1227.5               0.884\n",
      "           3             125 0.20       49.3         914              24.2              49.0                  6.0             1916             102.1               0.928\n",
      "           3             125 0.25       48.0         787              28.8              50.4                  5.0             1916             115.5               0.928\n",
      "           3             125 0.30       46.9         744              31.1              51.7                  5.0             2033             130.2               0.928\n",
      "           3             125 0.35       45.4         703              33.8              51.2                  5.0             2190             149.9               0.928\n",
      "           3             125 0.40       43.9         678              36.0              52.7                  5.0             2391             164.4               0.928\n",
      "           3             125 0.45       42.3         678              37.1              55.0                  5.0             2395             171.4               0.928\n",
      "           3             125 0.50       40.1         658              39.6              56.8                  5.0             3160             205.0               0.928\n",
      "           3             125 0.60       33.2         579              50.2              56.6                  5.0             3888             284.5               0.928\n",
      "           3             125 0.65       28.4         500              62.3              61.0                  4.0             6168             381.7               0.928\n",
      "           3             125 0.70       22.4         417              81.1              62.6                  4.0            10009             574.1               0.928\n",
      "\n",
      "================================================================================\n",
      "🏆 OPTIMAL PARAMETERS SET:\n",
      "================================================================================\n",
      "Dataset size: 43,542 items\n",
      "Ideal clusters (√n): 208\n",
      "min_samples: 3\n",
      "PCA Components: 125\n",
      "DBSCAN eps: 0.25\n",
      "Expected noise: 48.0%\n",
      "Expected clusters: 787\n",
      "Expected tiny cluster %: 50.4%\n",
      "Expected median cluster size: 5.0\n",
      "Expected largest cluster: 1916.0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test parameters (memory-safe approach)\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "print(\"Testing parameters...\")\n",
    "results = []\n",
    "\n",
    "# Test fewer combinations to avoid kernel crash\n",
    "for min_samp in [2, 3]:  # Just test 2 and 3\n",
    "    for n_components in [75, 100, 125]:  # Added 75, removed 150 to save memory\n",
    "        pca = PCA(n_components=n_components)\n",
    "        embeddings_reduced = pca.fit_transform(embeddings)\n",
    "        \n",
    "        for eps_val in [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.65, 0.7]:  # Added 0.35 and 0.45\n",
    "            algo = DBSCAN(min_samples=min_samp, eps=eps_val)\n",
    "            clusters = algo.fit_predict(embeddings_reduced)\n",
    "            \n",
    "            noise_pct = (clusters == -1).sum() / len(clusters) * 100\n",
    "            n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "            variance_explained = pca.explained_variance_ratio_.sum()\n",
    "            \n",
    "            # Calculate average cluster size\n",
    "            if n_clusters > 0:\n",
    "                avg_cluster_size = (len(clusters) - (clusters == -1).sum()) / n_clusters\n",
    "            else:\n",
    "                avg_cluster_size = 0\n",
    "            \n",
    "            # Calculate cluster size distribution metrics\n",
    "            if n_clusters > 0:\n",
    "                cluster_values = pd.Series(clusters)\n",
    "                cluster_sizes = cluster_values.value_counts()\n",
    "                cluster_sizes = cluster_sizes[cluster_sizes.index != -1]  # Exclude noise\n",
    "                \n",
    "                if len(cluster_sizes) > 0:\n",
    "                    tiny_clusters = (cluster_sizes <= 5).sum()\n",
    "                    tiny_pct = tiny_clusters / len(cluster_sizes) * 100\n",
    "                    median_cluster_size = cluster_sizes.median()\n",
    "                    largest_cluster = cluster_sizes.max()\n",
    "                    # Calculate cluster size standard deviation (measure of distribution balance)\n",
    "                    cluster_size_std = cluster_sizes.std()\n",
    "                else:\n",
    "                    tiny_pct = 0\n",
    "                    median_cluster_size = 0\n",
    "                    largest_cluster = 0\n",
    "                    cluster_size_std = 0\n",
    "            else:\n",
    "                tiny_pct = 0\n",
    "                median_cluster_size = 0\n",
    "                largest_cluster = 0\n",
    "                cluster_size_std = 0\n",
    "                \n",
    "            results.append({\n",
    "                'min_samples': min_samp,\n",
    "                'pca_components': n_components,\n",
    "                'eps': eps_val,\n",
    "                'noise_pct': round(noise_pct, 1),\n",
    "                'n_clusters': n_clusters,\n",
    "                'avg_cluster_size': round(avg_cluster_size, 1),\n",
    "                'tiny_cluster_pct': round(tiny_pct, 1),\n",
    "                'median_cluster_size': median_cluster_size,\n",
    "                'largest_cluster': largest_cluster,\n",
    "                'cluster_size_std': round(cluster_size_std, 1),\n",
    "                'variance_explained': round(variance_explained, 3)\n",
    "            })\n",
    "            \n",
    "            # Clean up memory after each test\n",
    "            del clusters, algo\n",
    "            gc.collect()\n",
    "        \n",
    "        # Clean up after each PCA test  \n",
    "        del embeddings_reduced, pca\n",
    "        gc.collect()\n",
    "\n",
    "# Create DataFrame and find optimal settings\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# GENERALIZED quality score - works across different dataset sizes and types\n",
    "total_items = len(embeddings)\n",
    "\n",
    "# Calculate relative thresholds based on dataset size\n",
    "ideal_clusters = max(10, min(1000, int(total_items ** 0.5)))  # Square root scaling\n",
    "ideal_avg_size = max(5, min(200, total_items / ideal_clusters))  # Reasonable cluster sizes\n",
    "\n",
    "results_df['quality_score'] = (\n",
    "    # Penalize extreme noise (too high OR too low) - use np.minimum instead of min\n",
    "    np.minimum(abs(results_df['noise_pct'] - 8), abs(results_df['noise_pct'] - 15)) * 2 +\n",
    "    \n",
    "    # Penalize very high tiny cluster percentage (over 50% is usually bad)\n",
    "    np.maximum(0, results_df['tiny_cluster_pct'] - 50) * 3 +\n",
    "    \n",
    "    # Penalize extreme cluster counts (too few or too many relative to data size)\n",
    "    (abs(results_df['n_clusters'] - ideal_clusters) / ideal_clusters) * 50 +\n",
    "    \n",
    "    # Prefer higher variance explained\n",
    "    (1 - results_df['variance_explained']) * 100 +\n",
    "    \n",
    "    # Penalize very unbalanced distributions (high std relative to mean)\n",
    "    np.where(results_df['avg_cluster_size'] > 0, \n",
    "             (results_df['cluster_size_std'] / results_df['avg_cluster_size']) * 20, 0) +\n",
    "    \n",
    "    # Penalize median cluster size being too small (indicates over-clustering)\n",
    "    np.maximum(0, 8 - results_df['median_cluster_size']) * 3 +\n",
    "    \n",
    "    # HEAVILY penalize monster clusters (over 1000 incidents is usually bad)\n",
    "    np.maximum(0, (results_df['largest_cluster'] - 1000) / 1000) * 100 +\n",
    "    \n",
    "    # Additional penalty for any cluster over 5% of total data\n",
    "    np.maximum(0, (results_df['largest_cluster'] / total_items * 100 - 5)) * 10\n",
    ")\n",
    "\n",
    "# Find and set optimal parameters\n",
    "best_idx = results_df['quality_score'].idxmin()\n",
    "best_settings = results_df.loc[best_idx]\n",
    "\n",
    "# AUTO-SET OPTIMAL PARAMETERS\n",
    "optimal_min_samples = int(best_settings['min_samples'])\n",
    "optimal_pca_components = int(best_settings['pca_components'])\n",
    "optimal_eps = best_settings['eps']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 OPTIMAL PARAMETERS SET:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset size: {total_items:,} items\")\n",
    "print(f\"Ideal clusters (√n): {ideal_clusters}\")\n",
    "print(f\"min_samples: {optimal_min_samples}\")\n",
    "print(f\"PCA Components: {optimal_pca_components}\")\n",
    "print(f\"DBSCAN eps: {optimal_eps}\")\n",
    "print(f\"Expected noise: {best_settings['noise_pct']}%\")\n",
    "print(f\"Expected clusters: {int(best_settings['n_clusters'])}\")\n",
    "print(f\"Expected tiny cluster %: {best_settings['tiny_cluster_pct']}%\")\n",
    "print(f\"Expected median cluster size: {best_settings['median_cluster_size']}\")\n",
    "print(f\"Expected largest cluster: {best_settings['largest_cluster']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba847ca4-9f5e-4591-b24e-8719c1d6a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster size statistics:\n",
      "Total clusters: 240\n",
      "Smallest cluster: 3\n",
      "Largest cluster: 15716\n",
      "Median cluster size: 4.0\n",
      "Mean cluster size: 152.7\n",
      "\n",
      "============================================================\n",
      "CLUSTER SIZE DISTRIBUTION:\n",
      "============================================================\n",
      "Tiny (2-5)      |  151 clusters ( 62.9%) ███████████████████████████████\n",
      "Small (6-20)    |   49 clusters ( 20.4%) ██████████\n",
      "Medium (21-100) |   20 clusters (  8.3%) ████\n",
      "Large (101-500) |   10 clusters (  4.2%) ██\n",
      "Huge (500+)     |   10 clusters (  4.2%) ██\n",
      "\n",
      "============================================================\n",
      "TOP 10 LARGEST CLUSTERS:\n",
      "============================================================\n",
      " 1. Cluster 2: 15,716 incidents\n",
      " 2. Cluster 0: 3,265 incidents\n",
      " 3. Cluster 4: 3,238 incidents\n",
      " 4. Cluster 8: 2,817 incidents\n",
      " 5. Cluster 5: 2,628 incidents\n",
      " 6. Cluster 1: 1,751 incidents\n",
      " 7. Cluster 9: 902 incidents\n",
      " 8. Cluster 3: 814 incidents\n",
      " 9. Cluster 14: 617 incidents\n",
      "10. Cluster 27: 617 incidents\n",
      "\n",
      "============================================================\n",
      "SAMPLE INCIDENTS FROM LARGEST CLUSTER (15716 incidents):\n",
      "============================================================\n",
      "1. CMBC-1-82 - Scheduling panel offline\n",
      "2. Aircon issue reported in Cambridge\n",
      "3. Wiki.arm.com is down\n",
      "4. Meeting Room – Issue - MUC-4-Starnberger See\n",
      "5. NAHPC2 - Unable to ssh from VM machine 10.252.156.254 to NAHPC2\n"
     ]
    }
   ],
   "source": [
    "# Text-based cluster size analysis (no imports needed)\n",
    "import pandas as pd\n",
    "\n",
    "# Get cluster sizes (excluding noise)\n",
    "cluster_sizes = df['cluster'].value_counts()\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != '-1']  # Exclude noise\n",
    "\n",
    "print(f\"Cluster size statistics:\")\n",
    "print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "print(f\"Smallest cluster: {cluster_sizes.min()}\")\n",
    "print(f\"Largest cluster: {cluster_sizes.max()}\")\n",
    "print(f\"Median cluster size: {cluster_sizes.median()}\")\n",
    "print(f\"Mean cluster size: {cluster_sizes.mean():.1f}\")\n",
    "\n",
    "# Create size buckets\n",
    "size_buckets = {\n",
    "    'Tiny (2-5)': (cluster_sizes <= 5).sum(),\n",
    "    'Small (6-20)': ((cluster_sizes > 5) & (cluster_sizes <= 20)).sum(),\n",
    "    'Medium (21-100)': ((cluster_sizes > 20) & (cluster_sizes <= 100)).sum(),\n",
    "    'Large (101-500)': ((cluster_sizes > 100) & (cluster_sizes <= 500)).sum(),\n",
    "    'Huge (500+)': (cluster_sizes > 500).sum()\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CLUSTER SIZE DISTRIBUTION:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for bucket, count in size_buckets.items():\n",
    "    pct = count / len(cluster_sizes) * 100\n",
    "    bar = '█' * int(pct / 2)  # Simple text bar chart\n",
    "    print(f\"{bucket:15} | {count:4d} clusters ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOP 10 LARGEST CLUSTERS:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "top_clusters = cluster_sizes.head(10)\n",
    "for i, (cluster_id, size) in enumerate(top_clusters.items(), 1):\n",
    "    print(f\"{i:2d}. Cluster {cluster_id}: {size:,} incidents\")\n",
    "\n",
    "# Show some example incidents from the largest cluster\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SAMPLE INCIDENTS FROM LARGEST CLUSTER ({top_clusters.iloc[0]} incidents):\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "largest_cluster_id = top_clusters.index[0]\n",
    "largest_cluster_incidents = df[df['cluster'] == largest_cluster_id]['freetext'].head(5)\n",
    "\n",
    "for i, incident in enumerate(largest_cluster_incidents, 1):\n",
    "    print(f\"{i}. {incident[:100]}...\" if len(incident) > 100 else f\"{i}. {incident}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f055748-a66b-47f2-b30f-774b19320d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal EPS Set: 0.7\n",
      "PCA explained variance ratio:  0.9275452759546976\n",
      "Embeddings shape: (43542, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run PCA with optimal parameters\n",
    "pca = PCA(n_components=optimal_pca_components)\n",
    "embeddings_reduced = pca.fit_transform(embeddings)\n",
    "print(f\"Optimal EPS Set: {optimal_eps}\")\n",
    "print(\"PCA explained variance ratio: \", pca.explained_variance_ratio_.sum())\n",
    "print(\"Embeddings shape:\", embeddings_reduced.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61330357-9149-4e7d-be35-1837bca21a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add features for digits to reduce false positive rate, i.e. each digit is a feature\n",
    "# addfeatures = df['freetext'].apply(lambda x: ','.join(re.findall(r'\\d', x))).str.split(',', expand=True).fillna(0).replace('',0).astype(int).values\n",
    "\n",
    "# NUMBERS_WEIGHTING = 5\n",
    "# embeddings_reduced = np.concatenate((embeddings_reduced[:,:-10], NUMBERS_WEIGHTING*addfeatures), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c893c933-46ca-41af-aaa2-9aa848657c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(embeddings_reduced, columns=[f'emb_{i}' for i in range(embeddings_reduced.shape[1])])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0c92b-10aa-4825-95d1-dab64a8b1dac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hack Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c6fa0ed-8e67-4322-9aa1-9901efbc1a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this section does the following: in case there is a column in the dataset that dictates two records to be considered the same, then their embeddings will be set equal\n",
    "# that way, they end up being in the same cluster\n",
    "# Example: records that have been validated by the customer to be in the same cluster will continue being in the same cluster in further iterations\n",
    "# so if there is a column called ValidationFlag that captures that, we would call hack_embeddings('ValidationFlag')\n",
    "\n",
    "# df = hack_embeddings('ManufacturerPartNumber')\n",
    "# df = hack_embeddings('OldMaterialNumber')\n",
    "# df = hack_embeddings('ValidationFlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60beb98f-8215-4100-ba62-050b46da4f87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal EPS Set: 0.75\n",
      "Optimal Min Samples Set: 2\n",
      "Nr. unique clusters: 648\n",
      "Total incidents: 43,542\n",
      "Incidents successfully clustered: 37,455 (86.0%)\n",
      "Noise incidents (outliers): 6,087 (14.0%)\n"
     ]
    }
   ],
   "source": [
    "### Run clustering algorithm: DBSCAN with optimal parameters\n",
    "print(f\"Optimal EPS Set: {optimal_eps}\")\n",
    "print(f\"Optimal Min Samples Set: {optimal_min_samples}\")\n",
    "algo = DBSCAN(min_samples=optimal_min_samples, eps=optimal_eps)\n",
    "clusters = algo.fit_predict(embeddings_reduced)\n",
    "# Add clusters to dataframe\n",
    "df['cluster'] = clusters.astype(str)\n",
    "df['pca1'] = embeddings_reduced[:,0]\n",
    "df['pca2'] = embeddings_reduced[:,1]\n",
    "# Calculate clustering statistics\n",
    "total_incidents = len(clusters)\n",
    "noise_incidents = (clusters == -1).sum()\n",
    "clustered_incidents = total_incidents - noise_incidents\n",
    "unique_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)  # Exclude noise cluster from count\n",
    "print(f\"Nr. unique clusters: {unique_clusters}\")\n",
    "print(f\"Total incidents: {total_incidents:,}\")\n",
    "print(f\"Incidents successfully clustered: {clustered_incidents:,} ({clustered_incidents/total_incidents*100:.1f}%)\")\n",
    "print(f\"Noise incidents (outliers): {noise_incidents:,} ({noise_incidents/total_incidents*100:.1f}%)\")\n",
    "df\n",
    "# Show potential duplicates\n",
    "cluster_multiple_materials = df['cluster'].value_counts().index[(df['cluster'].value_counts() > 1)]\n",
    "duplicates = df[df['cluster'].isin(cluster_multiple_materials)].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d7026-9402-49c9-b2b7-0de9ff530fe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **4. Push Data Back and Update DM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff5e17c1-ef92-4e11-b956-8d9fc20b87fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:40,758] WARNING: STRING columns are by default stored as VARCHAR(80) and therefore cut after 80 characters. You can specify a custom field length for each column using the `column_config` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:40,928] INFO: Successfully created data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n",
      "[2025-07-21 20:25:40,929] INFO: Add data frame as file chunks to data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d24511784384da9bbf63765cd958492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:41,340] INFO: Successfully upserted file chunk to data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n",
      "[2025-07-21 20:25:41,693] INFO: Successfully triggered execution for data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n",
      "[2025-07-21 20:25:41,694] INFO: Wait for execution of data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4099bca60f974291861d81a916ffd31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:47,888] INFO: Successfully created table 'incidentClusters' in data pool\n",
      "[2025-07-21 20:25:48,021] INFO: Successfully deleted data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n"
     ]
    }
   ],
   "source": [
    "table_name = 'incidentClusters'\n",
    "\n",
    "pool.create_table(\n",
    "    df.drop(columns=[col for col in df.columns if 'emb' in col]), \n",
    "    table_name=table_name, \n",
    "    drop_if_exists=True, \n",
    "    force=True)\n",
    "\n",
    "if UPDATE_DM:\n",
    "    dm.reload(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c46df-cfa5-447a-99cd-927d2ad5dd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39b666-b9ed-41a7-b957-9ad3dac8388a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default (Python 3.12.3)",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
