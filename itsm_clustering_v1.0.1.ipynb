{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0936b4-2bb4-4e88-994a-36d1ba38ae90",
   "metadata": {},
   "source": [
    "## **Incident Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67da0739-df90-41ae-b918-14d2f2837d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing to: /home/jovyan/.conda/envs/default/bin/python\n",
      "Packages will go to: /home/jovyan/.conda/envs/default/lib/python3.12/site-packages\n",
      "Looking in indexes: https://pypi.celonis.cloud, https://pypi.org/simple\n",
      "Collecting numpy==2.2.4\n",
      "  Using cached numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas==2.2.3\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting scikit-learn==1.6.1\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting sentence-transformers==4.1.0\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch==2.6.0\n",
      "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting transformers==4.51.3\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pycelonis==2.13.0\n",
      "  Using cached https://pypi.celonis.cloud/pycelonis/pycelonis-2.13.0-py3-none-any.whl (255 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.3)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.3)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.3)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.6.1)\n",
      "  Using cached scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.6.1)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.6.1)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tqdm (from sentence-transformers==4.1.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers==4.1.0)\n",
      "  Using cached huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence-transformers==4.1.0)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers==4.1.0)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from torch==2.6.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting networkx (from torch==2.6.0)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.6.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.6.0)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0)\n",
      "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting setuptools (from torch==2.6.0)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting packaging>=20.0 (from transformers==4.51.3)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.51.3)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.51.3)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers==4.51.3)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.51.3)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pycelonis-core>=2.10.3 (from pycelonis==2.13.0)\n",
      "  Using cached https://pypi.celonis.cloud/pycelonis-core/pycelonis_core-2.10.3-py3-none-any.whl (21 kB)\n",
      "Collecting pyarrow>=8.0.0 (from pycelonis==2.13.0)\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting saolapy>=0.3.3.dev0 (from pycelonis==2.13.0)\n",
      "  Using cached https://pypi.celonis.cloud/saolapy/saolapy-0.3.3-py3-none-any.whl (41 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers==4.1.0)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting httpx>=0.28.0 (from pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic (from pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.3)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.51.3)\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.51.3)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.51.3)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.51.3)\n",
      "  Using cached certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting anyio (from httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.28.0->pycelonis-core>=2.10.3->pycelonis==2.13.0)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "Using cached huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: triton, pytz, nvidia-cusparselt-cu12, mpmath, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, sympy, sniffio, six, setuptools, safetensors, regex, pyyaml, pyarrow, Pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, h11, fsspec, filelock, charset_normalizer, certifi, annotated-types, typing-inspection, scipy, requests, python-dateutil, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, httpcore, anyio, scikit-learn, pydantic, pandas, nvidia-cusolver-cu12, huggingface-hub, httpx, torch, tokenizers, saolapy, pycelonis-core, transformers, pycelonis, sentence-transformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.14.1\n",
      "    Uninstalling typing_extensions-4.14.1:\n",
      "      Successfully uninstalled typing_extensions-4.14.1\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.6.0\n",
      "    Uninstalling threadpoolctl-3.6.0:\n",
      "      Successfully uninstalled threadpoolctl-3.6.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 80.9.0\n",
      "    Uninstalling setuptools-80.9.0:\n",
      "      Successfully uninstalled setuptools-80.9.0\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 21.0.0\n",
      "    Uninstalling pyarrow-21.0.0:\n",
      "      Successfully uninstalled pyarrow-21.0.0\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 11.3.0\n",
      "    Uninstalling pillow-11.3.0:\n",
      "      Successfully uninstalled pillow-11.3.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.5\n",
      "    Uninstalling networkx-3.5:\n",
      "      Successfully uninstalled networkx-3.5\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.5.1\n",
      "    Uninstalling joblib-1.5.1:\n",
      "      Successfully uninstalled joblib-1.5.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.1.5\n",
      "    Uninstalling hf-xet-1.1.5:\n",
      "      Successfully uninstalled hf-xet-1.1.5\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.7.14\n",
      "    Uninstalling certifi-2025.7.14:\n",
      "      Successfully uninstalled certifi-2025.7.14\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: typing-inspection\n",
      "    Found existing installation: typing-inspection 0.4.1\n",
      "    Uninstalling typing-inspection-0.4.1:\n",
      "      Successfully uninstalled typing-inspection-0.4.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.1\n",
      "    Uninstalling scipy-1.16.1:\n",
      "      Successfully uninstalled scipy-1.16.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.2\n",
      "    Uninstalling pydantic_core-2.33.2:\n",
      "      Successfully uninstalled pydantic_core-2.33.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.9.0\n",
      "    Uninstalling anyio-4.9.0:\n",
      "      Successfully uninstalled anyio-4.9.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.7\n",
      "    Uninstalling pydantic-2.11.7:\n",
      "      Successfully uninstalled pydantic-2.11.7\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.34.1\n",
      "    Uninstalling huggingface-hub-0.34.1:\n",
      "      Successfully uninstalled huggingface-hub-0.34.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: saolapy\n",
      "    Found existing installation: saolapy 0.3.3\n",
      "    Uninstalling saolapy-0.3.3:\n",
      "      Successfully uninstalled saolapy-0.3.3\n",
      "  Attempting uninstall: pycelonis-core\n",
      "    Found existing installation: pycelonis_core 2.10.3\n",
      "    Uninstalling pycelonis_core-2.10.3:\n",
      "      Successfully uninstalled pycelonis_core-2.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: pycelonis\n",
      "    Found existing installation: pycelonis 2.13.0\n",
      "    Uninstalling pycelonis-2.13.0:\n",
      "      Successfully uninstalled pycelonis-2.13.0\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 4.1.0\n",
      "    Uninstalling sentence-transformers-4.1.0:\n",
      "      Successfully uninstalled sentence-transformers-4.1.0\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.3.0 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.7.14 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.7.0 h11-0.16.0 hf-xet-1.1.5 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.34.1 idna-3.10 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 numpy-2.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 packaging-25.0 pandas-2.2.3 pyarrow-21.0.0 pycelonis-2.13.0 pycelonis-core-2.10.3 pydantic-2.11.7 pydantic-core-2.33.2 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 saolapy-0.3.3 scikit-learn-1.6.1 scipy-1.16.1 sentence-transformers-4.1.0 setuptools-80.9.0 six-1.17.0 sniffio-1.3.1 sympy-1.13.1 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3 triton-3.2.0 typing-inspection-0.4.1 typing_extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNCOMMENT AND RUN THIS THE FIRST TIME YOUR RUN THE SCRIPT, THEN COMMENT IT BACK\n",
    "# Install packages to the correct location (3.12 site-packages)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Installing to: {sys.executable}\")\n",
    "print(\"Packages will go to: /home/jovyan/.conda/envs/default/lib/python3.12/site-packages\")\n",
    "\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'pip', 'install', '--force-reinstall',\n",
    "    'numpy==2.2.4',\n",
    "    'pandas==2.2.3', \n",
    "    'scikit-learn==1.6.1',\n",
    "    'sentence-transformers==4.1.0',\n",
    "    'torch==2.6.0',\n",
    "    'transformers==4.51.3',\n",
    "    'pycelonis==2.13.0'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a5939a-2607-4629-879c-a4253d05999c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from pycelonis import get_celonis\n",
    "import pycelonis.pql as pql\n",
    "from pycelonis.config import Config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b2036e-55b2-4f80-b992-b4636e24db43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_common_embedding(group):\n",
    "    if group.shape[0] < 2: \n",
    "        return group\n",
    "    common_embedding = group.iloc[0, df_number_of_cols:].values  # Use the first embedding in the group (all columns except 'ManufacturerPartNumber')\n",
    "    for i in range(embeddings_reduced.shape[1]):\n",
    "        group.iloc[:, df_number_of_cols+i] = common_embedding[i-1]\n",
    "    return group\n",
    "\n",
    "def hack_embeddings(column_name):\n",
    "    df_withcol = df[df[column_name].notna()]\n",
    "    df_withoutcol = df[df[column_name].isna()]\n",
    "    df_withcol = df_withcol.groupby(column_name).apply(assign_common_embedding).reset_index(drop=True)\n",
    "    return pd.concat((df_withcol, df_withoutcol), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62c3d8-aa8e-449f-b1a6-1d625d75ad54",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **1.Get Data from Celonis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb012b31-0bbd-44d0-ae3b-5b5153ab4c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-27 23:55:37,010] INFO: No `base_url` given. Using environment variable 'CELONIS_URL'\n",
      "[2025-07-27 23:55:37,011] INFO: No `api_token` given. Using environment variable 'CELONIS_API_TOKEN'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-27 23:55:37,195] WARNING: Your PyCelonis Version 2.13.0 is outdated (Newest Version: 2.14.0). Please upgrade the package via: pip install --extra-index-url=https://pypi.celonis.cloud/ pycelonis pycelonis_core --upgrade\n",
      "[2025-07-27 23:55:37,398] WARNING: KeyType is not set. Defaulted to 'APP_KEY'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-27 23:55:37,437] INFO: Initial connect successful! PyCelonis Version: 2.13.0\n",
      "[2025-07-27 23:55:37,531] INFO: `ml-workbench` permissions: ['DELETE_SCHEDULERS', 'EDIT_SCHEDULERS', 'USE_ALL_SCHEDULERS', 'CREATE_APPS', 'USE_ALL_APPS', 'CREATE_SCHEDULERS', 'CREATE_WORKSPACES', 'MANAGE_ALL_APPS', 'MANAGE_ALL_WORKSPACES', 'MANAGE_SCHEDULERS_PERMISSIONS', 'VIEW_CONFIGURATION']\n",
      "[2025-07-27 23:55:37,531] INFO: `team` permissions: []\n",
      "[2025-07-27 23:55:37,532] INFO: `process-repository` permissions: []\n",
      "[2025-07-27 23:55:37,532] INFO: `storage-manager` permissions: []\n",
      "[2025-07-27 23:55:37,532] INFO: `package-manager` permissions: []\n",
      "[2025-07-27 23:55:37,532] INFO: `transformation-center` permissions: []\n",
      "[2025-07-27 23:55:37,533] INFO: `compute-live` permissions: []\n",
      "[2025-07-27 23:55:37,533] INFO: `task-mining` permissions: []\n",
      "[2025-07-27 23:55:37,533] INFO: `event-collection` permissions: ['EDIT_ALL_DATA_POOLS_RESTRICTED', 'USE_ALL_DATA_MODELS', 'VIEW_ALL_DATA_POOLS', 'CREATE_DATA_POOL', 'EDIT_ALL_DATA_POOLS']\n",
      "[2025-07-27 23:55:37,534] INFO: `transformation-hub` permissions: []\n",
      "[2025-07-27 23:55:37,534] INFO: `user-provisioning` permissions: []\n",
      "[2025-07-27 23:55:37,534] INFO: `workflows` permissions: []\n"
     ]
    }
   ],
   "source": [
    "# Get Data from Celonis\n",
    "\n",
    "data_pool = 'OCPM Data Pool [ARM Multi-Object DEV]' #data pool where the data resides\n",
    "data_model = 'perspective_custom_ITSM'\n",
    "\n",
    "celonis = get_celonis()\n",
    "pool = celonis.data_integration.get_data_pools().find(data_pool)\n",
    "dm = pool.get_data_models().find(data_model)\n",
    "Config.DEFAULT_DATA_MODEL = dm\n",
    "\n",
    "# Choose if Data Model should be reloaded (True) or not (False)\n",
    "UPDATE_DM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21eb83d1-6556-49f1-a7d7-bcecf8f56939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Analysis:\n",
      "o_custom_RequestItem\n",
      "o_custom_Sla\n",
      "o_custom_AssignmentGroup\n",
      "o_custom_Incident\n",
      "o_custom_CatalogTask\n",
      "o_custom_Problem\n",
      "o_custom_ProblemTask\n",
      "o_custom_Problem__AssignmentGroup\n",
      "o_custom_CatalogTask__Sla\n",
      "o_custom_Cluster\n",
      "\n",
      "\n",
      "Column Analysis:\n",
      "o_custom_RequestItem columns (28 total):\n",
      "\n",
      "Cluster\n",
      "PrimaryCatalog\n",
      "Number\n",
      "RequestNumber\n",
      "AssignmentGroupName\n",
      "AssignedTo\n",
      "ReassignmentCount\n",
      "OpenedBy\n",
      "RequestedFor\n",
      "CreatedBy\n",
      "ClosedBy\n",
      "CatalogItem\n",
      "ConfigurationItem\n",
      "BusinessService\n",
      "ContactType\n",
      "Location\n",
      "ShortDescription\n",
      "Description\n",
      "Priority\n",
      "Impact\n",
      "Urgency\n",
      "DueDate\n",
      "Approval\n",
      "ApprovalSet\n",
      "State\n",
      "OpenedAt\n",
      "ClosedAt\n",
      "ID\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows in the DM except event tables, relationship tables and tables starting with _\n",
    "print(f\"Table Analysis:\")\n",
    "for tab in dm.get_tables():\n",
    "    if not (tab.name.startswith('e') or tab.name.startswith('r') or tab.name.startswith('_')):\n",
    "        print(tab.alias)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Input the tables aliases as they are in the DM\n",
    "table_aliases = ['o_custom_RequestItem']\n",
    "tables = {table_alias:[x for x in dm.get_tables() if x.alias == table_alias][0] for table_alias in table_aliases} \n",
    "tables\n",
    "\n",
    "# Inspect columns\n",
    "columns = {table_name:table.get_columns() for table_name,table in tables.items()}\n",
    "\n",
    "print(f\"Column Analysis:\")\n",
    "for table_name, cols in columns.items():\n",
    "    print(table_name + f\" columns ({len(cols)} total):\\n\")\n",
    "    for col in cols:\n",
    "        print(col.name)\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a652fee0-ff9b-4649-a0c9-2487ff942c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-27 23:55:39,189] INFO: Successfully created data export using api v1 with id '1c63ddf0-8b9a-4626-bbc3-9a0df3b65571'\n",
      "[2025-07-27 23:55:39,189] INFO: Wait for execution of data export with id '1c63ddf0-8b9a-4626-bbc3-9a0df3b65571'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038a99c9b3934c8dbfa5d219de7f8abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-27 23:55:39,269] INFO: Export result chunks for data export with id '1c63ddf0-8b9a-4626-bbc3-9a0df3b65571'\n",
      "\n",
      "\n",
      "Memory usage:  0.20 MB\n",
      "Nr. of records: 12489\n",
      "Nr. of unique materials: 12489\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>freetext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170cc3c761b5ea1040babaa27115c89c</td>\n",
       "      <td>Please give access to remote windows machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171eed501b82d2102380404cd34bcbc8</td>\n",
       "      <td>Need access to AI office ServiceNow group - Query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17c7ffe51b48de907abf9828b04bcb3f</td>\n",
       "      <td>Request for Apple Business Manager access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17ccd1029f0226107ed453e76724ab68</td>\n",
       "      <td>Assistance in the Multiverse navigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18bce85c1b1f5a108fb2337f034bcbf0</td>\n",
       "      <td>How to reverse a free-issuing of stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18c78226c384a6501c44d8ed05013150</td>\n",
       "      <td>Assistance for MS Project installation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18ccdadac3b466d8661799fb0501319f</td>\n",
       "      <td>Service Account nnx_mbed is not reflecting in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18da26e483182ed08a4fed60ceaad3d8</td>\n",
       "      <td>Set up OOO Automatic reply Agisti Reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>196ba89083c7a2d03d06e8b52bda1ecd</td>\n",
       "      <td>Multiverse  - Would Like to Request a Rule to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>196c70621bdb161468398557d34bcb24</td>\n",
       "      <td>Tech Hub on call rota</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0  170cc3c761b5ea1040babaa27115c89c   \n",
       "1  171eed501b82d2102380404cd34bcbc8   \n",
       "2  17c7ffe51b48de907abf9828b04bcb3f   \n",
       "3  17ccd1029f0226107ed453e76724ab68   \n",
       "4  18bce85c1b1f5a108fb2337f034bcbf0   \n",
       "5  18c78226c384a6501c44d8ed05013150   \n",
       "6  18ccdadac3b466d8661799fb0501319f   \n",
       "7  18da26e483182ed08a4fed60ceaad3d8   \n",
       "8  196ba89083c7a2d03d06e8b52bda1ecd   \n",
       "9  196c70621bdb161468398557d34bcb24   \n",
       "\n",
       "                                            freetext  \n",
       "0      Please give access to remote windows machine   \n",
       "1  Need access to AI office ServiceNow group - Query  \n",
       "2          Request for Apple Business Manager access  \n",
       "3            Assistance in the Multiverse navigation  \n",
       "4             How to reverse a free-issuing of stock  \n",
       "5             Assistance for MS Project installation  \n",
       "6  Service Account nnx_mbed is not reflecting in ...  \n",
       "7            Set up OOO Automatic reply Agisti Reddy  \n",
       "8  Multiverse  - Would Like to Request a Rule to ...  \n",
       "9                              Tech Hub on call rota  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pql_filter = '''FILTER UPPER(\"o_custom_RequestItem\".\"CatalogItem\") LIKE ('%NON-CATALOG IT%') AND DAYS_BETWEEN(\"o_custom_RequestItem\".\"OpenedAt\", MINUTE_NOW()) <= 365'''    # change this if you want to include a filter\n",
    "\n",
    "# there are two ways to get data: either via simple columns/attributes or via custom PQL\n",
    "# simple column example: columns['MARA'].find(\"MANDT\") would give you MARA.MANDT\n",
    "df_meta = pql.DataFrame(\n",
    "    data=\n",
    "    {\n",
    "        \"ID\": columns['o_custom_RequestItem'].find(\"ID\")\n",
    "        ,\"freetext\": columns['o_custom_RequestItem'].find(\"ShortDescription\")\n",
    "    },\n",
    "    filters=\n",
    "    {pql_filter}\n",
    ")\n",
    "\n",
    "try:\n",
    "    df = df_meta.to_pandas().reset_index(drop=True)\n",
    "except:\n",
    "    df = pd.read_csv('materials-dataset.csv')\n",
    "    df.columns = ['ID','freetext']\n",
    "    \n",
    "df = df.drop_duplicates().dropna(subset=['ID'])\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1e6: .2f} MB\")\n",
    "print(\"Nr. of records:\", len(df))\n",
    "print(\"Nr. of unique materials:\", len(df[\"ID\"].unique()))\n",
    "print(f\"\\n\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde7c5f-71f7-4295-af93-e0029cb47835",
   "metadata": {},
   "source": [
    "#### **2. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d973bad3-4f71-4c8a-b7ae-e7acd6450ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare freetext column to be fed to the LLM (i.e. set the freetext column as a concatentation of columns from the data above, use the column names as in row[column_name])\n",
    "df[\"freetext\"] = df.apply(lambda row: f\"{row['freetext']}\", axis=1)\n",
    "df['freetext'] = df['freetext'].fillna('')\n",
    "#df\n",
    "\n",
    "# Needed for correct concatenation of columns later\n",
    "df_number_of_cols = df.shape[1]\n",
    "#df_number_of_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c9e74-2b28-404d-af94-6e9a8d8174b1",
   "metadata": {},
   "source": [
    "#### **3. Clustering using Embeddings + Additional Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e94b48-d934-414f-a1b0-0b5f2aba3836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3093be3783fd4f9d80a89ee5c4f10f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape:  torch.Size([12489, 768])\n"
     ]
    }
   ],
   "source": [
    "# Run transformer\n",
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\")  # previously: all-MiniLM-L6-v2\n",
    "embeddings = model.encode(df.freetext, batch_size=256, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(\"Embeddings shape: \", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92eb5cb4-bd3b-446f-af00-81325f0197a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters...\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING RESULTS:\n",
      "================================================================================\n",
      " min_samples  pca_components  eps  noise_pct  n_clusters  avg_cluster_size  tiny_cluster_pct  median_cluster_size  largest_cluster  cluster_size_std  variance_explained\n",
      "           2              75 0.20       85.4         588               3.1              92.2                  2.0               30               3.1               0.753\n",
      "           2              75 0.25       84.1         621               3.2              90.2                  2.0               31               3.2               0.753\n",
      "           2              75 0.30       82.2         679               3.3              90.0                  2.0               46               3.6               0.753\n",
      "           2              75 0.35       79.8         757               3.3              90.0                  2.0               46               3.8               0.753\n",
      "           2              75 0.40       76.9         843               3.4              89.4                  2.0               50               4.1               0.753\n",
      "           2              75 0.45       73.1         929               3.6              88.5                  2.0               51               4.6               0.753\n",
      "           2              75 0.50       68.1        1015               3.9              87.5                  2.0               67               5.6               0.753\n",
      "           2              75 0.60       50.4         981               6.3              85.7                  2.0             1139              38.6               0.753\n",
      "           2              75 0.65       37.9         708              11.0              87.1                  2.0             4911             184.5               0.753\n",
      "           2              75 0.70       21.8         402              24.3              91.3                  2.0             8450             421.3               0.753\n",
      "           2             100 0.20       86.2         565               3.0              92.2                  2.0               27               2.8               0.839\n",
      "           2             100 0.25       84.9         604               3.1              90.7                  2.0               30               3.1               0.839\n",
      "           2             100 0.30       83.4         646               3.2              89.8                  2.0               31               3.2               0.839\n",
      "           2             100 0.35       81.6         690               3.3              89.4                  2.0               46               3.7               0.839\n",
      "           2             100 0.40       79.2         770               3.4              89.6                  2.0               46               3.9               0.839\n",
      "           2             100 0.45       76.3         839               3.5              89.3                  2.0               50               4.4               0.839\n",
      "           2             100 0.50       72.7         933               3.7              88.1                  2.0               51               4.7               0.839\n",
      "           2             100 0.60       61.4        1016               4.7              85.0                  2.0              191               9.9               0.839\n",
      "           2             100 0.65       52.4         922               6.5              85.5                  2.0             1484              49.7               0.839\n",
      "           2             100 0.70       41.1         717              10.3              87.0                  2.0             4460             166.5               0.839\n",
      "           2             125 0.20       86.4         559               3.0              92.5                  2.0               27               2.8               0.900\n",
      "           2             125 0.25       85.4         590               3.1              92.0                  2.0               30               3.1               0.900\n",
      "           2             125 0.30       84.1         619               3.2              90.1                  2.0               31               3.2               0.900\n",
      "           2             125 0.35       82.5         669               3.3              90.1                  2.0               46               3.6               0.900\n",
      "           2             125 0.40       80.4         734               3.3              89.6                  2.0               46               3.8               0.900\n",
      "           2             125 0.45       78.0         799               3.4              89.6                  2.0               46               4.1               0.900\n",
      "           2             125 0.50       74.8         879               3.6              89.0                  2.0               50               4.6               0.900\n",
      "           2             125 0.60       65.9         986               4.3              85.2                  2.0              161               7.6               0.900\n",
      "           2             125 0.65       58.9         994               5.2              84.3                  2.0              379              14.8               0.900\n",
      "           2             125 0.70       49.7         836               7.5              86.6                  2.0             2874              99.5               0.900\n",
      "           3              75 0.20       92.0         180               5.6              74.4                  4.0               30               4.7               0.753\n",
      "           3              75 0.25       90.9         200               5.7              69.5                  4.0               31               4.7               0.753\n",
      "           3              75 0.30       89.4         231               5.7              70.6                  4.0               46               5.3               0.753\n",
      "           3              75 0.35       87.8         257               5.9              70.4                  4.0               46               5.6               0.753\n",
      "           3              75 0.40       85.6         299               6.0              70.2                  4.0               50               6.1               0.753\n",
      "           3              75 0.45       82.4         352               6.2              69.6                  4.0               51               6.6               0.753\n",
      "           3              75 0.50       77.9         405               6.8              68.6                  4.0               67               8.1               0.753\n",
      "           3              75 0.60       59.3         425              12.0              67.1                  4.0             1139              58.2               0.753\n",
      "           3              75 0.65       44.7         283              24.4              67.8                  4.0             4911             291.7               0.753\n",
      "           3              75 0.70       25.7         157              59.1              77.7                  4.0             8450             674.0               0.753\n",
      "           3             100 0.20       92.5         171               5.5              74.3                  4.0               27               4.3               0.839\n",
      "           3             100 0.25       91.6         184               5.7              69.6                  4.0               30               4.6               0.839\n",
      "           3             100 0.30       90.4         208               5.8              68.3                  4.0               31               4.7               0.839\n",
      "           3             100 0.35       88.8         238               5.9              69.3                  4.0               46               5.5               0.839\n",
      "           3             100 0.40       87.2         269               5.9              70.3                  4.0               46               5.8               0.839\n",
      "           3             100 0.45       84.9         307               6.2              70.7                  4.0               50               6.5               0.839\n",
      "           3             100 0.50       82.0         354               6.4              68.6                  4.0               51               6.8               0.839\n",
      "           3             100 0.60       70.8         431               8.5              64.7                  4.0              191              14.4               0.839\n",
      "           3             100 0.65       60.6         406              12.1              67.0                  4.0             1484              74.5               0.839\n",
      "           3             100 0.70       47.7         299              21.8              68.9                  4.0             4460             257.7               0.839\n",
      "           3             125 0.20       92.6         170               5.4              75.3                  4.0               27               4.3               0.900\n",
      "           3             125 0.25       92.0         178               5.6              73.6                  4.0               30               4.7               0.900\n",
      "           3             125 0.30       90.9         198               5.7              69.2                  4.0               31               4.7               0.900\n",
      "           3             125 0.35       89.6         226               5.8              70.8                  4.0               46               5.3               0.900\n",
      "           3             125 0.40       88.0         256               5.8              70.3                  4.0               46               5.6               0.900\n",
      "           3             125 0.45       86.2         289               6.0              71.3                  4.0               46               6.1               0.900\n",
      "           3             125 0.50       83.6         324               6.3              70.1                  4.0               50               6.7               0.900\n",
      "           3             125 0.60       75.1         413               7.5              64.6                  4.0              161              10.9               0.900\n",
      "           3             125 0.65       68.1         424               9.4              63.2                  4.0              379              22.0               0.900\n",
      "           3             125 0.70       57.5         349              15.2              67.9                  4.0             2874             153.7               0.900\n",
      "\n",
      "================================================================================\n",
      "🏆 OPTIMAL PARAMETERS SET:\n",
      "================================================================================\n",
      "Dataset size: 12,489 items\n",
      "Ideal clusters (√n): 111\n",
      "min_samples: 3\n",
      "PCA Components: 125\n",
      "DBSCAN eps: 0.3\n",
      "Expected noise: 90.9%\n",
      "Expected clusters: 198\n",
      "Expected tiny cluster %: 69.2%\n",
      "Expected median cluster size: 4.0\n",
      "Expected largest cluster: 31.0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test parameters (memory-safe approach)\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "print(\"Testing parameters...\")\n",
    "results = []\n",
    "\n",
    "# Test fewer combinations to avoid kernel crash\n",
    "for min_samp in [2, 3]:  # Just test 2 and 3\n",
    "    for n_components in [75, 100, 125]:  # Added 75, removed 150 to save memory\n",
    "        pca = PCA(n_components=n_components)\n",
    "        embeddings_reduced = pca.fit_transform(embeddings)\n",
    "        \n",
    "        for eps_val in [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.65, 0.7]:  # Added 0.35 and 0.45\n",
    "            algo = DBSCAN(min_samples=min_samp, eps=eps_val)\n",
    "            clusters = algo.fit_predict(embeddings_reduced)\n",
    "            \n",
    "            noise_pct = (clusters == -1).sum() / len(clusters) * 100\n",
    "            n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "            variance_explained = pca.explained_variance_ratio_.sum()\n",
    "            \n",
    "            # Calculate average cluster size\n",
    "            if n_clusters > 0:\n",
    "                avg_cluster_size = (len(clusters) - (clusters == -1).sum()) / n_clusters\n",
    "            else:\n",
    "                avg_cluster_size = 0\n",
    "            \n",
    "            # Calculate cluster size distribution metrics\n",
    "            if n_clusters > 0:\n",
    "                cluster_values = pd.Series(clusters)\n",
    "                cluster_sizes = cluster_values.value_counts()\n",
    "                cluster_sizes = cluster_sizes[cluster_sizes.index != -1]  # Exclude noise\n",
    "                \n",
    "                if len(cluster_sizes) > 0:\n",
    "                    tiny_clusters = (cluster_sizes <= 5).sum()\n",
    "                    tiny_pct = tiny_clusters / len(cluster_sizes) * 100\n",
    "                    median_cluster_size = cluster_sizes.median()\n",
    "                    largest_cluster = cluster_sizes.max()\n",
    "                    # Calculate cluster size standard deviation (measure of distribution balance)\n",
    "                    cluster_size_std = cluster_sizes.std()\n",
    "                else:\n",
    "                    tiny_pct = 0\n",
    "                    median_cluster_size = 0\n",
    "                    largest_cluster = 0\n",
    "                    cluster_size_std = 0\n",
    "            else:\n",
    "                tiny_pct = 0\n",
    "                median_cluster_size = 0\n",
    "                largest_cluster = 0\n",
    "                cluster_size_std = 0\n",
    "                \n",
    "            results.append({\n",
    "                'min_samples': min_samp,\n",
    "                'pca_components': n_components,\n",
    "                'eps': eps_val,\n",
    "                'noise_pct': round(noise_pct, 1),\n",
    "                'n_clusters': n_clusters,\n",
    "                'avg_cluster_size': round(avg_cluster_size, 1),\n",
    "                'tiny_cluster_pct': round(tiny_pct, 1),\n",
    "                'median_cluster_size': median_cluster_size,\n",
    "                'largest_cluster': largest_cluster,\n",
    "                'cluster_size_std': round(cluster_size_std, 1),\n",
    "                'variance_explained': round(variance_explained, 3)\n",
    "            })\n",
    "            \n",
    "            # Clean up memory after each test\n",
    "            del clusters, algo\n",
    "            gc.collect()\n",
    "        \n",
    "        # Clean up after each PCA test  \n",
    "        del embeddings_reduced, pca\n",
    "        gc.collect()\n",
    "\n",
    "# Create DataFrame and find optimal settings\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# GENERALIZED quality score - works across different dataset sizes and types\n",
    "total_items = len(embeddings)\n",
    "\n",
    "# Calculate relative thresholds based on dataset size\n",
    "ideal_clusters = max(10, min(1000, int(total_items ** 0.5)))  # Square root scaling\n",
    "ideal_avg_size = max(5, min(200, total_items / ideal_clusters))  # Reasonable cluster sizes\n",
    "\n",
    "results_df['quality_score'] = (\n",
    "    # Penalize extreme noise (too high OR too low) - use np.minimum instead of min\n",
    "    np.minimum(abs(results_df['noise_pct'] - 8), abs(results_df['noise_pct'] - 15)) * 2 +\n",
    "    \n",
    "    # Penalize very high tiny cluster percentage (over 50% is usually bad)\n",
    "    np.maximum(0, results_df['tiny_cluster_pct'] - 50) * 3 +\n",
    "    \n",
    "    # Penalize extreme cluster counts (too few or too many relative to data size)\n",
    "    (abs(results_df['n_clusters'] - ideal_clusters) / ideal_clusters) * 50 +\n",
    "    \n",
    "    # Prefer higher variance explained\n",
    "    (1 - results_df['variance_explained']) * 100 +\n",
    "    \n",
    "    # Penalize very unbalanced distributions (high std relative to mean)\n",
    "    np.where(results_df['avg_cluster_size'] > 0, \n",
    "             (results_df['cluster_size_std'] / results_df['avg_cluster_size']) * 20, 0) +\n",
    "    \n",
    "    # Penalize median cluster size being too small (indicates over-clustering)\n",
    "    np.maximum(0, 8 - results_df['median_cluster_size']) * 3 +\n",
    "    \n",
    "    # HEAVILY penalize monster clusters (over 1000 incidents is usually bad)\n",
    "    np.maximum(0, (results_df['largest_cluster'] - 1000) / 1000) * 100 +\n",
    "    \n",
    "    # Additional penalty for any cluster over 5% of total data\n",
    "    np.maximum(0, (results_df['largest_cluster'] / total_items * 100 - 5)) * 10\n",
    ")\n",
    "\n",
    "# Find and set optimal parameters\n",
    "best_idx = results_df['quality_score'].idxmin()\n",
    "best_settings = results_df.loc[best_idx]\n",
    "\n",
    "# AUTO-SET OPTIMAL PARAMETERS\n",
    "optimal_min_samples = int(best_settings['min_samples'])\n",
    "optimal_pca_components = int(best_settings['pca_components'])\n",
    "optimal_eps = best_settings['eps']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 OPTIMAL PARAMETERS SET:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset size: {total_items:,} items\")\n",
    "print(f\"Ideal clusters (√n): {ideal_clusters}\")\n",
    "print(f\"min_samples: {optimal_min_samples}\")\n",
    "print(f\"PCA Components: {optimal_pca_components}\")\n",
    "print(f\"DBSCAN eps: {optimal_eps}\")\n",
    "print(f\"Expected noise: {best_settings['noise_pct']}%\")\n",
    "print(f\"Expected clusters: {int(best_settings['n_clusters'])}\")\n",
    "print(f\"Expected tiny cluster %: {best_settings['tiny_cluster_pct']}%\")\n",
    "print(f\"Expected median cluster size: {best_settings['median_cluster_size']}\")\n",
    "print(f\"Expected largest cluster: {best_settings['largest_cluster']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f055748-a66b-47f2-b30f-774b19320d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal EPS Set: 0.3\n",
      "PCA explained variance ratio:  0.8997188299674871\n",
      "Embeddings shape: (12489, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5748"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run PCA with optimal parameters\n",
    "pca = PCA(n_components=optimal_pca_components)\n",
    "embeddings_reduced = pca.fit_transform(embeddings)\n",
    "print(f\"Optimal EPS Set: {optimal_eps}\")\n",
    "print(\"PCA explained variance ratio: \", pca.explained_variance_ratio_.sum())\n",
    "print(\"Embeddings shape:\", embeddings_reduced.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61330357-9149-4e7d-be35-1837bca21a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add features for digits to reduce false positive rate, i.e. each digit is a feature\n",
    "# addfeatures = df['freetext'].apply(lambda x: ','.join(re.findall(r'\\d', x))).str.split(',', expand=True).fillna(0).replace('',0).astype(int).values\n",
    "\n",
    "# NUMBERS_WEIGHTING = 5\n",
    "# embeddings_reduced = np.concatenate((embeddings_reduced[:,:-10], NUMBERS_WEIGHTING*addfeatures), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893c933-46ca-41af-aaa2-9aa848657c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(embeddings_reduced, columns=[f'emb_{i}' for i in range(embeddings_reduced.shape[1])])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0c92b-10aa-4825-95d1-dab64a8b1dac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hack Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fa0ed-8e67-4322-9aa1-9901efbc1a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this section does the following: in case there is a column in the dataset that dictates two records to be considered the same, then their embeddings will be set equal\n",
    "# that way, they end up being in the same cluster\n",
    "# Example: records that have been validated by the customer to be in the same cluster will continue being in the same cluster in further iterations\n",
    "# so if there is a column called ValidationFlag that captures that, we would call hack_embeddings('ValidationFlag')\n",
    "\n",
    "# df = hack_embeddings('ManufacturerPartNumber')\n",
    "# df = hack_embeddings('OldMaterialNumber')\n",
    "# df = hack_embeddings('ValidationFlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60beb98f-8215-4100-ba62-050b46da4f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run clustering algorithm: DBSCAN with optimal parameters\n",
    "print(f\"Optimal EPS Set: {optimal_eps}\")\n",
    "print(f\"Optimal Min Samples Set: {optimal_min_samples}\")\n",
    "algo = DBSCAN(min_samples=optimal_min_samples, eps=optimal_eps)\n",
    "clusters = algo.fit_predict(embeddings_reduced)\n",
    "# Add clusters to dataframe\n",
    "df['cluster'] = clusters.astype(str)\n",
    "df['pca1'] = embeddings_reduced[:,0]\n",
    "df['pca2'] = embeddings_reduced[:,1]\n",
    "# Calculate clustering statistics\n",
    "total_incidents = len(clusters)\n",
    "noise_incidents = (clusters == -1).sum()\n",
    "clustered_incidents = total_incidents - noise_incidents\n",
    "unique_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)  # Exclude noise cluster from count\n",
    "print(f\"Nr. unique clusters: {unique_clusters}\")\n",
    "print(f\"Total incidents: {total_incidents:,}\")\n",
    "print(f\"Incidents successfully clustered: {clustered_incidents:,} ({clustered_incidents/total_incidents*100:.1f}%)\")\n",
    "print(f\"Noise incidents (outliers): {noise_incidents:,} ({noise_incidents/total_incidents*100:.1f}%)\")\n",
    "df\n",
    "# Show potential duplicates\n",
    "cluster_multiple_materials = df['cluster'].value_counts().index[(df['cluster'].value_counts() > 1)]\n",
    "duplicates = df[df['cluster'].isin(cluster_multiple_materials)].sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f0ffa-c9fa-44ef-b44e-a4339359682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-based cluster size analysis (no imports needed)\n",
    "import pandas as pd\n",
    "\n",
    "# Get cluster sizes (excluding noise)\n",
    "cluster_sizes = df['cluster'].value_counts()\n",
    "cluster_sizes = cluster_sizes[cluster_sizes.index != '-1']  # Exclude noise\n",
    "\n",
    "print(f\"Cluster size statistics:\")\n",
    "print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "print(f\"Smallest cluster: {cluster_sizes.min()}\")\n",
    "print(f\"Largest cluster: {cluster_sizes.max()}\")\n",
    "print(f\"Median cluster size: {cluster_sizes.median()}\")\n",
    "print(f\"Mean cluster size: {cluster_sizes.mean():.1f}\")\n",
    "\n",
    "# Create size buckets\n",
    "size_buckets = {\n",
    "    'Tiny (2-5)': (cluster_sizes <= 5).sum(),\n",
    "    'Small (6-20)': ((cluster_sizes > 5) & (cluster_sizes <= 20)).sum(),\n",
    "    'Medium (21-100)': ((cluster_sizes > 20) & (cluster_sizes <= 100)).sum(),\n",
    "    'Large (101-500)': ((cluster_sizes > 100) & (cluster_sizes <= 500)).sum(),\n",
    "    'Huge (500+)': (cluster_sizes > 500).sum()\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CLUSTER SIZE DISTRIBUTION:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for bucket, count in size_buckets.items():\n",
    "    pct = count / len(cluster_sizes) * 100\n",
    "    bar = '█' * int(pct / 2)  # Simple text bar chart\n",
    "    print(f\"{bucket:15} | {count:4d} clusters ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOP 10 LARGEST CLUSTERS:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "top_clusters = cluster_sizes.head(10)\n",
    "for i, (cluster_id, size) in enumerate(top_clusters.items(), 1):\n",
    "    print(f\"{i:2d}. Cluster {cluster_id}: {size:,} incidents\")\n",
    "\n",
    "# Show some example incidents from the largest cluster\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SAMPLE INCIDENTS FROM LARGEST CLUSTER ({top_clusters.iloc[0]} incidents):\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "largest_cluster_id = top_clusters.index[0]\n",
    "largest_cluster_incidents = df[df['cluster'] == largest_cluster_id]['freetext'].head(5)\n",
    "\n",
    "for i, incident in enumerate(largest_cluster_incidents, 1):\n",
    "    print(f\"{i}. {incident[:100]}...\" if len(incident) > 100 else f\"{i}. {incident}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d7026-9402-49c9-b2b7-0de9ff530fe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **4. Push Data Back and Update DM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff5e17c1-ef92-4e11-b956-8d9fc20b87fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:40,758] WARNING: STRING columns are by default stored as VARCHAR(80) and therefore cut after 80 characters. You can specify a custom field length for each column using the `column_config` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:40,928] INFO: Successfully created data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n",
      "[2025-07-21 20:25:40,929] INFO: Add data frame as file chunks to data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d24511784384da9bbf63765cd958492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:41,340] INFO: Successfully upserted file chunk to data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n",
      "[2025-07-21 20:25:41,693] INFO: Successfully triggered execution for data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n",
      "[2025-07-21 20:25:41,694] INFO: Wait for execution of data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4099bca60f974291861d81a916ffd31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-21 20:25:47,888] INFO: Successfully created table 'incidentClusters' in data pool\n",
      "[2025-07-21 20:25:48,021] INFO: Successfully deleted data push job with id 'e4f64c2f-0b2b-410f-81a3-ad1153fab0e7'\n"
     ]
    }
   ],
   "source": [
    "table_name = 'CatalogItemClusters'\n",
    "\n",
    "pool.create_table(\n",
    "    df.drop(columns=[col for col in df.columns if 'emb' in col]), \n",
    "    table_name=table_name, \n",
    "    drop_if_exists=True, \n",
    "    force=True)\n",
    "\n",
    "if UPDATE_DM:\n",
    "    dm.reload(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c46df-cfa5-447a-99cd-927d2ad5dd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39b666-b9ed-41a7-b957-9ad3dac8388a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default (Python 3.12.3)",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
